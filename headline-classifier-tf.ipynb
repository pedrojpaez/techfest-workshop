{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end NLP: News Headline classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup execution role and session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "4c1a412b-4535-46d5-8fe3-e8600801817a",
    "_uuid": "4e6801037e5274668f0b8667591d41c1abbe8be1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting. If you don't specify a bucket, SageMaker SDK will create a default bucket following a pre-defined naming convention in the same region. \n",
    "- The IAM role ARN used to give SageMaker access to your data. It can be fetched using the **get_execution_role** method from sagemaker python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::349934754982:role/service-role/AmazonSageMaker-ExecutionRole-20190410T152971\n",
      "CPU times: user 542 ms, sys: 28.8 ms, total: 571 ms\n",
      "Wall time: 1.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download News Aggregator Dataset available at the public UCI dataset repository (these files should already be downloaded in previous notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip NewsAggregatorDataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf __MACOSX/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize the dataset\n",
    "\n",
    "We will load the newsCorpora.csv file to a Pandas dataframe for our data processing work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow\n",
    "import re\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>URL</th>\n",
       "      <th>PUBLISHER</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>STORY</th>\n",
       "      <th>HOSTNAME</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fed official says weak data caused by weather,...</td>\n",
       "      <td>http://www.latimes.com/business/money/la-fi-mo...</td>\n",
       "      <td>Los Angeles Times</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.latimes.com</td>\n",
       "      <td>1394470370698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fed's Charles Plosser sees high bar for change...</td>\n",
       "      <td>http://www.livemint.com/Politics/H2EvwJSK2VE6O...</td>\n",
       "      <td>Livemint</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.livemint.com</td>\n",
       "      <td>1394470371207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US open: Stocks fall after Fed official hints ...</td>\n",
       "      <td>http://www.ifamagazine.com/news/us-open-stocks...</td>\n",
       "      <td>IFA Magazine</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.ifamagazine.com</td>\n",
       "      <td>1394470371550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fed risks falling 'behind the curve', Charles ...</td>\n",
       "      <td>http://www.ifamagazine.com/news/fed-risks-fall...</td>\n",
       "      <td>IFA Magazine</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.ifamagazine.com</td>\n",
       "      <td>1394470371793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fed's Plosser: Nasty Weather Has Curbed Job Gr...</td>\n",
       "      <td>http://www.moneynews.com/Economy/federal-reser...</td>\n",
       "      <td>Moneynews</td>\n",
       "      <td>b</td>\n",
       "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
       "      <td>www.moneynews.com</td>\n",
       "      <td>1394470372027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TITLE  \\\n",
       "1  Fed official says weak data caused by weather,...   \n",
       "2  Fed's Charles Plosser sees high bar for change...   \n",
       "3  US open: Stocks fall after Fed official hints ...   \n",
       "4  Fed risks falling 'behind the curve', Charles ...   \n",
       "5  Fed's Plosser: Nasty Weather Has Curbed Job Gr...   \n",
       "\n",
       "                                                 URL          PUBLISHER  \\\n",
       "1  http://www.latimes.com/business/money/la-fi-mo...  Los Angeles Times   \n",
       "2  http://www.livemint.com/Politics/H2EvwJSK2VE6O...           Livemint   \n",
       "3  http://www.ifamagazine.com/news/us-open-stocks...       IFA Magazine   \n",
       "4  http://www.ifamagazine.com/news/fed-risks-fall...       IFA Magazine   \n",
       "5  http://www.moneynews.com/Economy/federal-reser...          Moneynews   \n",
       "\n",
       "  CATEGORY                          STORY             HOSTNAME      TIMESTAMP  \n",
       "1        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM      www.latimes.com  1394470370698  \n",
       "2        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM     www.livemint.com  1394470371207  \n",
       "3        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM  www.ifamagazine.com  1394470371550  \n",
       "4        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM  www.ifamagazine.com  1394470371793  \n",
       "5        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM    www.moneynews.com  1394470372027  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = [\"TITLE\", \"URL\", \"PUBLISHER\", \"CATEGORY\", \"STORY\", \"HOSTNAME\", \"TIMESTAMP\"]\n",
    "news_dataset = pd.read_csv('newsCorpora.csv', names=column_names, header=None, delimiter='\\t')\n",
    "news_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For this exercice we'll only use the title (Headline) of the news story and the category as our target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=news_dataset[['TITLE',\"CATEGORY\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'b': 115967, 't': 108344, 'e': 152469, 'm': 45639})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(df['CATEGORY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has four categories: Business (b), Science & Technology (t), Entertainment (e) and Health & Medicine (m)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language pre processing\n",
    "\n",
    "We will do some basic processing of the text data to convert it into numerical form that the algorithm will be able to consume to create a model.\n",
    "We will do typical pre processing for NLP workloads such as: dummy encoding the labels, tokenizing the documents and set fixed sequence lengths for input feature dimension, padding documents to have fixed length input vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from keras.utils.np_utils import to_categorical\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "docs = df[\"TITLE\"].values\n",
    "\n",
    "encoder.fit(df[\"CATEGORY\"].values)\n",
    "encoded_Y = encoder.transform(df[\"CATEGORY\"].values)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bucket = <bucket> # custom bucket name.\n",
    "s3_bucket = sess.default_bucket()\n",
    "s3_prefix = 'news'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b', 'e', 'm', 't']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize documents and set fixed sequence lengths for input feature dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "7bcf422f-0e75-4d49-b3b1-12553fcaf4ff",
    "_uuid": "46b7fc9aef5a519f96a295e980ba15deee781e97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 75286\n",
      "Number of headlines: 422419\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(\"Vocabulary size: \" + str(vocab_size))\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 40\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(\"Number of headlines: \" + str(len(padded_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fed official says weak data caused by weather, should not slow taper'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectors.txt file is the output of the blazingtext_word2vec_text8.ipynb notebook. This will have a list of vector representations for each word in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 71291 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('./vectors.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "97ec3b51-53be-4078-a72a-a4222d2ffac0",
    "_uuid": "72b42e2a27337810e4604db0f67d626a62008854"
   },
   "outputs": [],
   "source": [
    "#print(t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "fa875535-7cef-40da-9add-dfe1d51395b0",
    "_uuid": "befd8941982ee2119daa0b9cc6b10a1e14656239"
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir ./data/ ./data/embeddings/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save embedding matrix to push to S3 for Sagemaker to use during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "593bfd0a-b703-4e87-96dd-a7eb98e6940e",
    "_uuid": "f71c5f0b731d3418d3cb83be758233b5030da29d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75286, 100)\n"
     ]
    }
   ],
   "source": [
    "#embedding_matrix.dump(\"ingredients-embedding-matrix.dat\")\n",
    "np.save(file=\"./data/embeddings/docs-embedding-matrix\",\n",
    "        arr=embedding_matrix,\n",
    "        allow_pickle=False)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will prep the data for ingestion for the algortihm. Split the data set in train and test samples and uplad the data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs, dummy_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data/train/ data/test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data/train/train_X.npy', X_train)\n",
    "np.save('./data/train/train_Y.npy', y_train)\n",
    "np.save('./data/test/test_X.npy', X_test)\n",
    "np.save('./data/test/test_Y.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_s3_prefix = '{}/data/train'.format(s3_prefix)\n",
    "testdata_s3_prefix = '{}/data/test'.format(s3_prefix)\n",
    "embeddings_s3_prefix='{}/data/embeddings'.format(s3_prefix)\n",
    "output_s3 = 's3://{}/{}/models/'.format(s3_bucket, s3_prefix)\n",
    "code_location_s3 = 's3://{}/{}/codes'.format(s3_bucket, s3_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s3 = sess.upload_data(path='./data/train/', bucket=s3_bucket, key_prefix=traindata_s3_prefix)\n",
    "test_s3 = sess.upload_data(path='./data/test/', bucket=s3_bucket, key_prefix=testdata_s3_prefix)\n",
    "embeddings_s3 = sess.upload_data(path='./data/embeddings/', bucket=s3_bucket, key_prefix=embeddings_s3_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 's3://sagemaker-ap-southeast-1-349934754982/news/data/train', 'test': 's3://sagemaker-ap-southeast-1-349934754982/news/data/test', 'embeddings': 's3://sagemaker-ap-southeast-1-349934754982/news/data/embeddings'}\n"
     ]
    }
   ],
   "source": [
    "inputs = {'train':train_s3, 'test': test_s3, 'embeddings': embeddings_s3}\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker training with differentiated infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the high level MXNet SDK to train our MXNet model using Sagemaker. We have packaged the code we used to build and train our model in the previous notebook (headline-classifier-local.ipynb). The training script is available in the tf-src directory. You can read the details of the script and format in the python file: keras_script_mxnet.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hyperparameters to push to algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'epochs': 5, 'vocab_size':vocab_size, 'num_classes':encoder.classes_.size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our `TensorFlow` estimator object, we have set the hyper-parameters for this object and we have our data channels linked with the algorithm. The only  remaining thing to do is to train the algorithm. The following command will train the algorithm. Training the algorithm involves a few steps. Firstly, the instance that we requested while creating the `TensorFlow` estimator classes is provisioned and is setup with the appropriate libraries. Then, the data from our channels are downloaded into the instance. Once this is done, the training job begins. The provisioning and data downloading will take some time, depending on the size of the data. \n",
    "\n",
    "Once the job has finished a \"Job complete\" message will be printed. The trained model can be found in the S3 bucket that was setup as `output_path` in the estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run the training job using a ml.p2.xlarge instance (GPUs) to accelerate our training. If your account runs into resource limits please use a ml.c5.xlarge instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-tensorflow-scriptmode-2019-04-22-11-22-51-058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-22 11:22:54 Starting - Starting the training job...\n",
      "2019-04-22 11:22:56 Starting - Launching requested ML instances...\n",
      "2019-04-22 11:23:34 Starting - Insufficient capacity error from EC2 while launching instances, retrying!.........\n",
      "2019-04-22 11:25:07 Starting - Preparing the instances for training...\n",
      "2019-04-22 11:25:53 Downloading - Downloading input data...\n",
      "2019-04-22 11:26:11 Training - Downloading the training image..\n",
      "\u001b[32m2019-04-22 11:26:33,755 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[32m2019-04-22 11:26:34,038 sagemaker_tensorflow_container.training INFO     Running distributed training job with parameter servers\u001b[0m\n",
      "\u001b[32m2019-04-22 11:26:34,039 sagemaker_tensorflow_container.training INFO     Launching parameter server process\u001b[0m\n",
      "\u001b[32m2019-04-22 11:26:34,039 sagemaker_tensorflow_container.training INFO     Running distributed training job with parameter servers\u001b[0m\n",
      "\u001b[32m2019-04-22 11:26:36,055 sagemaker_tensorflow_container.training INFO     Launching worker process\u001b[0m\n",
      "\u001b[31m2019-04-22 11:26:35,273 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[31m2019-04-22 11:26:35,547 sagemaker_tensorflow_container.training INFO     Running distributed training job with parameter servers\u001b[0m\n",
      "\u001b[31m2019-04-22 11:26:35,548 sagemaker_tensorflow_container.training INFO     Launching parameter server process\u001b[0m\n",
      "\u001b[31m2019-04-22 11:26:35,548 sagemaker_tensorflow_container.training INFO     Running distributed training job with parameter servers\u001b[0m\n",
      "\u001b[32m2019-04-22 11:26:36,266 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[32mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[32m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_parameter_server_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"embeddings\": \"/opt/ml/input/data/embeddings\",\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"num_classes\": 4,\n",
      "        \"vocab_size\": 75286,\n",
      "        \"model_dir\": \"s3://sagemaker-ap-southeast-1-349934754982/sagemaker-tensorflow-scriptmode-2019-04-22-11-22-51-058/model\",\n",
      "        \"epochs\": 5\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"embeddings\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"sagemaker-tensorflow-scriptmode-2019-04-22-11-22-51-058\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-southeast-1-349934754982/sagemaker-tensorflow-scriptmode-2019-04-22-11-22-51-058/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"keras_script\",\n",
      "    \"network_interface_name\": \"ethwe\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 4,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"ethwe\"\n",
      "    },\n",
      "    \"user_entry_point\": \"keras_script.py\"\u001b[0m\n",
      "\u001b[32m}\n",
      "\u001b[0m\n",
      "\u001b[32mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[32mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[32mSM_NETWORK_INTERFACE_NAME=ethwe\u001b[0m\n",
      "\u001b[32mSM_HPS={\"epochs\":5,\"model_dir\":\"s3://sagemaker-ap-southeast-1-349934754982/sagemaker-tensorflow-scriptmode-2019-04-22-11-22-51-058/model\",\"num_classes\":4,\"vocab_size\":75286}\u001b[0m\n",
      "\u001b[32mSM_USER_ENTRY_POINT=keras_script.py\u001b[0m\n",
      "\u001b[32mSM_FRAMEWORK_PARAMS={\"sagemaker_parameter_server_enabled\":true}\u001b[0m\n",
      "\u001b[32mSM_RESOURCE_CONFIG={\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"ethwe\"}\u001b[0m\n",
      "\u001b[32mSM_INPUT_DATA_CONFIG={\"embeddings\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[32mSM_CHANNELS=[\"embeddings\",\"test\",\"train\"]\u001b[0m\n",
      "\u001b[32mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[32mSM_MODULE_NAME=keras_script\u001b[0m\n",
      "\u001b[32mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[32mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[32mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[32mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[32mSM_NUM_CPUS=32\u001b[0m\n",
      "\u001b[32mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[32mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[32mSM_MODULE_DIR=s3://sagemaker-ap-southeast-1-349934754982/sagemaker-tensorflow-scriptmode-2019-04-22-11-22-51-058/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[32mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_parameter_server_enabled\":true},\"channel_input_dirs\":{\"embeddings\":\"/opt/ml/input/data/embeddings\",\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"epochs\":5,\"model_dir\":\"s3://sagemaker-ap-southeast-1-349934754982/sagemaker-tensorflow-scriptmode-2019-04-22-11-22-51-058/model\",\"num_classes\":4,\"vocab_size\":75286},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"embeddings\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"sagemaker-tensorflow-scriptmode-2019-04-22-11-22-51-058\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-southeast-1-349934754982/sagemaker-tensorflow-scriptmode-2019-04-22-11-22-51-058/source/sourcedir.tar.gz\",\"module_name\":\"keras_script\",\"network_interface_name\":\"ethwe\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"ethwe\"},\"user_entry_point\":\"keras_script.py\"}\u001b[0m\n",
      "\u001b[32mSM_USER_ARGS=[\"--epochs\",\"5\",\"--model_dir\",\"s3://sagemaker-ap-southeast-1-349934754982/sagemaker-tensorflow-scriptmode-2019-04-22-11-22-51-058/model\",\"--num_classes\",\"4\",\"--vocab_size\",\"75286\"]\u001b[0m\n",
      "\u001b[32mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[32mSM_CHANNEL_EMBEDDINGS=/opt/ml/input/data/embeddings\u001b[0m\n",
      "\u001b[32mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[32mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[32mSM_HP_NUM_CLASSES=4\u001b[0m\n",
      "\u001b[32mSM_HP_VOCAB_SIZE=75286\u001b[0m\n",
      "\u001b[32mSM_HP_MODEL_DIR=s3://sagemaker-ap-southeast-1-349934754982/sagemaker-tensorflow-scriptmode-2019-04-22-11-22-51-058/model\u001b[0m\n",
      "\u001b[32mSM_HP_EPOCHS=5\n",
      "\u001b[0m\n",
      "\u001b[32mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[32m/usr/bin/python keras_script.py --epochs 5 --model_dir s3://sagemaker-ap-southeast-1-349934754982/sagemaker-tensorflow-scriptmode-2019-04-22-11-22-51-058/model --num_classes 4 --vocab_size 75286\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31m2019-04-22 11:26:37,567 sagemaker_tensorflow_container.training INFO     Launching worker process\u001b[0m\n",
      "\u001b[32mUsing TensorFlow backend.\u001b[0m\n",
      "\u001b[32mNamespace(embeddings='/opt/ml/input/data/embeddings', epochs=5, model_dir='/opt/ml/model', num_classes=4, output_data_dir='/opt/ml/output/data', test='/opt/ml/input/data/test', train='/opt/ml/input/data/train', vocab_size=75286)\u001b[0m\n",
      "\u001b[31m2019-04-22 11:26:37,793 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_parameter_server_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"embeddings\": \"/opt/ml/input/data/embeddings\",\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"num_classes\": 4,\n",
      "        \"vocab_size\": 75286,\n",
      "        \"model_dir\": \"s3://sagemaker-ap-southeast-1-349934754982/sagemaker-tensorflow-scriptmode-2019-04-22-11-22-51-058/model\",\n",
      "        \"epochs\": 5\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"embeddings\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-tensorflow-scriptmode-2019-04-22-11-22-51-058\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-southeast-1-349934754982/sagemaker-tensorflow-scriptmode-2019-04-22-11-22-51-058/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"keras_script\",\n",
      "    \"network_interface_name\": \"ethwe\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 4,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"ethwe\"\n",
      "    },\n",
      "    \"user_entry_point\": \"keras_script.py\"\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=ethwe\u001b[0m\n",
      "\u001b[31mSM_HPS={\"epochs\":5,\"model_dir\":\"s3://sagemaker-ap-southeast-1-349934754982/sagemaker-tensorflow-scriptmode-2019-04-22-11-22-51-058/model\",\"num_classes\":4,\"vocab_size\":75286}\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=keras_script.py\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={\"sagemaker_parameter_server_enabled\":true}\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"ethwe\"}\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"embeddings\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"embeddings\",\"test\",\"train\"]\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=keras_script\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=32\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-ap-southeast-1-349934754982/sagemaker-tensorflow-scriptmode-2019-04-22-11-22-51-058/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_parameter_server_enabled\":true},\"channel_input_dirs\":{\"embeddings\":\"/opt/ml/input/data/embeddings\",\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"epochs\":5,\"model_dir\":\"s3://sagemaker-ap-southeast-1-349934754982/sagemaker-tensorflow-scriptmode-2019-04-22-11-22-51-058/model\",\"num_classes\":4,\"vocab_size\":75286},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"embeddings\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-tensorflow-scriptmode-2019-04-22-11-22-51-058\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-southeast-1-349934754982/sagemaker-tensorflow-scriptmode-2019-04-22-11-22-51-058/source/sourcedir.tar.gz\",\"module_name\":\"keras_script\",\"network_interface_name\":\"ethwe\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"ethwe\"},\"user_entry_point\":\"keras_script.py\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--epochs\",\"5\",\"--model_dir\",\"s3://sagemaker-ap-southeast-1-349934754982/sagemaker-tensorflow-scriptmode-2019-04-22-11-22-51-058/model\",\"--num_classes\",\"4\",\"--vocab_size\",\"75286\"]\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_EMBEDDINGS=/opt/ml/input/data/embeddings\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31mSM_HP_NUM_CLASSES=4\u001b[0m\n",
      "\u001b[31mSM_HP_VOCAB_SIZE=75286\u001b[0m\n",
      "\u001b[31mSM_HP_MODEL_DIR=s3://sagemaker-ap-southeast-1-349934754982/sagemaker-tensorflow-scriptmode-2019-04-22-11-22-51-058/model\u001b[0m\n",
      "\u001b[31mSM_HP_EPOCHS=5\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/bin/python keras_script.py --epochs 5 --model_dir s3://sagemaker-ap-southeast-1-349934754982/sagemaker-tensorflow-scriptmode-2019-04-22-11-22-51-058/model --num_classes 4 --vocab_size 75286\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mUsing TensorFlow backend.\u001b[0m\n",
      "\u001b[31mNamespace(embeddings='/opt/ml/input/data/embeddings', epochs=5, model_dir='/opt/ml/model', num_classes=4, output_data_dir='/opt/ml/output/data', test='/opt/ml/input/data/test', train='/opt/ml/input/data/train', vocab_size=75286)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m_________________________________________________________________\u001b[0m\n",
      "\u001b[32mLayer (type)                 Output Shape              Param #   \u001b[0m\n",
      "\u001b[32m=================================================================\u001b[0m\n",
      "\u001b[32membed (Embedding)            (None, 40, 100)           7528600   \u001b[0m\n",
      "\u001b[32m_________________________________________________________________\u001b[0m\n",
      "\u001b[32mconv_1 (Conv1D)              (None, 38, 128)           38528     \u001b[0m\n",
      "\u001b[32m_________________________________________________________________\u001b[0m\n",
      "\u001b[32mmaxpool_1 (MaxPooling1D)     (None, 7, 128)            0         \u001b[0m\n",
      "\u001b[32m_________________________________________________________________\u001b[0m\n",
      "\u001b[32mflat_1 (Flatten)             (None, 896)               0         \u001b[0m\n",
      "\u001b[32m_________________________________________________________________\u001b[0m\n",
      "\u001b[32mdropout_1 (Dropout)          (None, 896)               0         \u001b[0m\n",
      "\u001b[32m_________________________________________________________________\u001b[0m\n",
      "\u001b[32mdense_1 (Dense)              (None, 128)               114816    \u001b[0m\n",
      "\u001b[32m_________________________________________________________________\u001b[0m\n",
      "\u001b[32mout_1 (Dense)                (None, 4)                 516       \u001b[0m\n",
      "\u001b[32m=================================================================\u001b[0m\n",
      "\u001b[32mTotal params: 7,682,460\u001b[0m\n",
      "\u001b[32mTrainable params: 153,860\u001b[0m\n",
      "\u001b[32mNon-trainable params: 7,528,600\u001b[0m\n",
      "\u001b[32m_________________________________________________________________\u001b[0m\n",
      "\u001b[32mEpoch 1/5\u001b[0m\n",
      "\u001b[31m_________________________________________________________________\u001b[0m\n",
      "\u001b[31mLayer (type)                 Output Shape              Param #   \u001b[0m\n",
      "\u001b[31m=================================================================\u001b[0m\n",
      "\u001b[31membed (Embedding)            (None, 40, 100)           7528600   \u001b[0m\n",
      "\u001b[31m_________________________________________________________________\u001b[0m\n",
      "\u001b[31mconv_1 (Conv1D)              (None, 38, 128)           38528     \u001b[0m\n",
      "\u001b[31m_________________________________________________________________\u001b[0m\n",
      "\u001b[31mmaxpool_1 (MaxPooling1D)     (None, 7, 128)            0         \u001b[0m\n",
      "\u001b[31m_________________________________________________________________\u001b[0m\n",
      "\u001b[31mflat_1 (Flatten)             (None, 896)               0         \u001b[0m\n",
      "\u001b[31m_________________________________________________________________\u001b[0m\n",
      "\u001b[31mdropout_1 (Dropout)          (None, 896)               0         \u001b[0m\n",
      "\u001b[31m_________________________________________________________________\u001b[0m\n",
      "\u001b[31mdense_1 (Dense)              (None, 128)               114816    \u001b[0m\n",
      "\u001b[31m_________________________________________________________________\u001b[0m\n",
      "\u001b[31mout_1 (Dense)                (None, 4)                 516       \u001b[0m\n",
      "\u001b[31m=================================================================\u001b[0m\n",
      "\u001b[31mTotal params: 7,682,460\u001b[0m\n",
      "\u001b[31mTrainable params: 153,860\u001b[0m\n",
      "\u001b[31mNon-trainable params: 7,528,600\u001b[0m\n",
      "\u001b[31m_________________________________________________________________\u001b[0m\n",
      "\u001b[31mEpoch 1/5\u001b[0m\n",
      "\n",
      "2019-04-22 11:26:59 Training - Training image download completed. Training in progress.\u001b[32m - 70s - loss: 0.2014 - acc: 0.9223\u001b[0m\n",
      "\u001b[32mEpoch 2/5\u001b[0m\n",
      "\u001b[31m - 71s - loss: 0.2014 - acc: 0.9221\u001b[0m\n",
      "\u001b[31mEpoch 2/5\u001b[0m\n",
      "\u001b[32m - 61s - loss: 0.1784 - acc: 0.9340\u001b[0m\n",
      "\u001b[32mEpoch 3/5\u001b[0m\n",
      "\u001b[31m - 62s - loss: 0.1778 - acc: 0.9341\u001b[0m\n",
      "\u001b[31mEpoch 3/5\u001b[0m\n",
      "\u001b[32m - 61s - loss: 0.1771 - acc: 0.9370\u001b[0m\n",
      "\u001b[32mEpoch 4/5\u001b[0m\n",
      "\u001b[31m - 62s - loss: 0.1768 - acc: 0.9370\u001b[0m\n",
      "\u001b[31mEpoch 4/5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tf_estimator = TensorFlow(entry_point='keras_script.py',\n",
    "                       source_dir='./tf-src',\n",
    "                        role=role,\n",
    "                        train_instance_count=2, \n",
    "                        train_instance_type='ml.p3.8xlarge',\n",
    "                        framework_version='1.11', py_version='py3',\n",
    "                        distributions={'parameter_server': {'enabled': True}},\n",
    "                        hyperparameters=hyperparameters,\n",
    "                         metric_definitions=[\n",
    "                           {'Name': 'validation:loss', 'Regex': 'loss: ([0-9\\\\.]+)'},\n",
    "                           {'Name': 'validation:accuracy', 'Regex': 'acc: ([0-9\\\\.]+)'}]\n",
    "                        )\n",
    "tf_estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the training job is running take a minute to look at the keras_script_mxnet.py script. You can see how we have adapted the our original local code from headline-classifier-mxnet.ipynb to run on Sagemaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting / Inference\n",
    "Once the training is done, we can deploy the trained model as an Amazon SageMaker real-time hosted endpoint. This will allow us to make predictions (or inference) from the model. Note that we don't have to host on the same type of instance that we used to train. Because instance endpoints will be up and running for long, it's advisable to choose a cheaper instance for inference.\n",
    "\n",
    "We will deploy our model using the MXNetModel object that will recieve as input the model object output by the training job to S3. We can use the different features of Sagemaker as decoupled modules. You can bring a model trained outside of Sagemaker and deploy it using the Sagemaker model deployment api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "key = mxnet_estimator.model_data[mxnet_estimator.model_data.find(\"/\", 5)+1:]\n",
    "s3.Bucket(s3_bucket).download_file(key, 'model.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path='model.tar.gz'\n",
    "from sagemaker.mxnet import MXNet, MXNetModel\n",
    "\n",
    "sagemaker_model = MXNetModel(model_data = model_path,\n",
    "                             role = role,\n",
    "                             entry_point = 'default_classifier.py',\n",
    "                             py_version='py3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = mxnet_estimator.deploy(initial_instance_count=1,\n",
    "                           instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your model should now be in production as a RESTful API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(predictor.accept, predictor.content_type, predictor.deserializer, predictor.endpoint, predictor.sagemaker_session, predictor.serializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate our model with an example headline. You can be creative with your headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_doc=['Senate prepares to vote on dueling plans to end shutdown']\n",
    "# integer encode the document\n",
    "encoded_example = t.texts_to_sequences(example_doc)\n",
    "\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 40\n",
    "padded_example = pad_sequences(encoded_example, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result represents the probability or confidence of the model that the headline represents one of the classes: 'b', 'e', 'm', 't'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(padded_example.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPO - Automatic Hyperparameter tuning (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter_ranges = {'learning_rate': ContinuousParameter(0.01, 0.2)}\n",
    "hyperparameter_ranges = {'epochs': IntegerParameter(2, 7)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'loss'\n",
    "objective_type = 'Minimize'\n",
    "metric_definitions = [{'Name': 'loss',\n",
    "                       'Regex': 'loss = ([0-9\\\\.]+)'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'epochs': 5, 'vocab_size':vocab_size, 'num_classes':encoder.classes_.size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxnet_estimator = MXNet(entry_point='keras_script_mxnet.py',\n",
    "                       source_dir='./tf-src',\n",
    "                        role=role,\n",
    "                        train_instance_type='ml.c5.xlarge',\n",
    "                        train_instance_count=1,\n",
    "                        framework_version='1.3.0',\n",
    "                        py_version='py3',\n",
    "                        hyperparameters=hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(mxnet_estimator,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            metric_definitions,\n",
    "                            max_jobs=5,\n",
    "                            max_parallel_jobs=2,\n",
    "                            objective_type=objective_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to Sagemaker console and under Training > Hyperparameter Tuning jobs > select the only entry. You will see different training jobs  with model configurations as well as a \"Best Training job\" tab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
